{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Написати програмний код який визначає частини мови для кожного слова у\n",
        "введеному реченні (Де DT – артикль, JJ – прикметник, NN – іменник, VBZ –\n",
        "дієслово тощо)"
      ],
      "metadata": {
        "id": "is5CHrsAZS8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')                      # Для токенізації\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2V_6STwYUG6",
        "outputId": "0e86b554-fc20-48da-9ab6-b2881655fc94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eL6t36voXs9w"
      },
      "outputs": [],
      "source": [
        "def pos_tagging(text):\n",
        "    words = nltk.word_tokenize(text)             # Токенізуємо текст на слова\n",
        "    tagged = nltk.pos_tag(words)                  # Тегуємо кожне слово\n",
        "\n",
        "    result = ' '.join([f\"{word}/{tag}\" for word, tag in tagged])\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_text = input(\"Введіть речення: \")\n",
        "\n",
        "# Викликаємо функцію\n",
        "pos_tagging(user_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIraYhi8YA5y",
        "outputId": "398e6105-037b-461e-af27-ee95c3107db9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введіть речення: Text to check this task\n",
            "Text/NN to/TO check/VB this/DT task/NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Написати програмний код, яка перекладає введений текст англійською (якщо він\n",
        "українською або іншою мовою)."
      ],
      "metadata": {
        "id": "uJlwDIQsZbnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n",
        "from googletrans import Translator, LANGUAGES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkOUTxgfYo5i",
        "outputId": "f6387f8e-a400-4025-a53a-ab8fd1ce9b0d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=a06baf76cae14a528f6694491004b06a16b95beefe4b66f0e170c254445e37d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.8\n",
            "    Uninstalling httpcore-1.0.8:\n",
            "      Successfully uninstalled httpcore-1.0.8\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.75.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.11.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.33 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_english(text):\n",
        "  translator = Translator()\n",
        "  try:\n",
        "    # Визначення мови тексту\n",
        "    detection = translator.detect(text)\n",
        "    detected_language = detection.lang\n",
        "\n",
        "    # Перевірка, чи мова тексту вже англійська\n",
        "    if detected_language == 'en':\n",
        "      return f\"Текст вже англійською: {text}\"\n",
        "    else:\n",
        "      # Переклад на англійську\n",
        "      translation = translator.translate(text, dest='en')\n",
        "      return f\"Оригінальна мова: {LANGUAGES.get(detected_language, detected_language).capitalize()}\\nПереклад англійською: {translation.text}\"\n",
        "  except Exception as e:\n",
        "    return f\"Виникла помилка під час перекладу: {e}\""
      ],
      "metadata": {
        "id": "HzCH8nWVZp11"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Введіть текст для перекладу: \")\n",
        "\n",
        "translated_text = translate_to_english(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f8rOeKXZw-1",
        "outputId": "ca98bbac-6428-4698-abe7-bf290c265ace"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введіть текст для перекладу: Це просто текст для перекладу на англійську\n",
            "Оригінальна мова: Ukrainian\n",
            "Переклад англійською: It's just a text to translate into English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Написати програмний код, яка генерує n-грами (фрази з n слів) із заданого тексту."
      ],
      "metadata": {
        "id": "WTG-Hm78Z4_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "  if not text or n < 1:\n",
        "    return []\n",
        "\n",
        "  # Токенізація тексту на слова\n",
        "  tokens = word_tokenize(text.lower()) # Переводимо до нижнього регістру для послідовності\n",
        "\n",
        "  # Генерація n-грам\n",
        "  n_grams_list = list(ngrams(tokens, n))\n",
        "\n",
        "  # Форматування n-грам як рядків (як у прикладі)\n",
        "  ngram_phrases = [\" \".join(gram) for gram in n_grams_list]\n",
        "\n",
        "  return ngram_phrases"
      ],
      "metadata": {
        "id": "7z-p0jgOZ6or"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing.\"\n",
        "n_value = 2\n",
        "\n",
        "ngrams_result = generate_ngrams(text, n_value)\n",
        "print(f\"Оригінальний текст: '{text}'\")\n",
        "print(f\"{n_value}-грами: {ngrams_result}\")\n",
        "\n",
        "text_2 = \"This is a simple example sentence.\"\n",
        "n_value_2 = 3\n",
        "\n",
        "ngrams_result_2 = generate_ngrams(text_2, n_value_2)\n",
        "print(f\"\\nОригінальний текст: '{text_2}'\")\n",
        "print(f\"{n_value_2}-грами: {ngrams_result_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVeOHBLsaEq8",
        "outputId": "4e89749d-a944-4e2a-8094-8ceeeba2b942"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оригінальний текст: 'I love natural language processing.'\n",
            "2-грами: ['i love', 'love natural', 'natural language', 'language processing', 'processing .']\n",
            "\n",
            "Оригінальний текст: 'This is a simple example sentence.'\n",
            "3-грами: ['this is a', 'is a simple', 'a simple example', 'simple example sentence', 'example sentence .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Написати програмний код, яка підсумовує довгі тексти, залишаючи лише основний зміст"
      ],
      "metadata": {
        "id": "uHbo_62OaP40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo0SMidYaRDa",
        "outputId": "055df8b6-452a-40f5-dfb5-7e1b65ffa5e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=989f01856567b47632f2e95ab1c22206a6fb1295e09066b1783b8a19184302a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=06f23aa5ebf00944739e38c5384e4bf30ad0a3c1d6336d52d6e588f7ab23f00b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, language=\"ukrainian\", sentences_count=5):\n",
        "  if not text:\n",
        "    return \"Вхідний текст порожній.\"\n",
        "\n",
        "  try:\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
        "    summarizer = LexRankSummarizer()\n",
        "\n",
        "    # Додавання стеммера та стоп-слів (опціонально, але покращує результат)\n",
        "    # Для української мови потрібно перевірити наявність стеммера в sumy\n",
        "    # Якщо стеммер для вказаної мови недоступний, ці рядки можна закоментувати або обробити виняток\n",
        "    try:\n",
        "        stemmer = Stemmer(language)\n",
        "        summarizer.stemmer = stemmer\n",
        "        # Завантаження стоп-слів для вказаної мови\n",
        "        # sumy підтримує обмежений набір мов для стоп-слів\n",
        "        # Якщо для вашої мови немає стоп-слів, цей рядок може викликати помилку\n",
        "        summarizer.stop_words = get_stop_words(language)\n",
        "    except LookupError:\n",
        "        print(f\"Попередження: Стеммер або стоп-слова для мови '{language}' не знайдені в sumy. Підсумовування може бути менш точним.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Попередження при налаштуванні стеммера/стоп-слів: {e}\")\n",
        "\n",
        "\n",
        "    # Генерація підсумку\n",
        "    summary = summarizer(parser.document, sentences_count=sentences_count)\n",
        "\n",
        "    # Об'єднання речень підсумку в один рядок\n",
        "    summarized_text = \" \".join([str(sentence) for sentence in summary])\n",
        "\n",
        "    return summarized_text\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Виникла помилка під час підсумовування: {e}\"\n",
        "\n",
        "# --- Приклад використання ---\n",
        "\n",
        "long_text_ukrainian = \"\"\"\n",
        "Україна - найбільша за площею країна Східної Європи, розташована в південно-східній частині континенту.\n",
        "Її територія охоплює значну частину Східноєвропейської рівнини, частину Карпатських гір та Кримського півострова.\n",
        "Україна має вихід до Чорного та Азовського морів.\n",
        "Межує з сімома країнами: Польщею, Словаччиною, Угорщиною, Румунією, Молдовою, Росією та Білоруссю.\n",
        "Столицею та найбільшим містом України є Київ.\n",
        "Країна має багату історію, яка сягає часів Київської Русі.\n",
        "Сучасна Україна здобула незалежність у 1991 році після розпаду Радянського Союзу.\n",
        "Економіка України є змішаною, з розвиненим сільським господарством та промисловістю.\n",
        "Останніми роками Україна активно прагне до інтеграції з Європейським Союзом та НАТО.\n",
        "Це прагнення зустрічає значний спротив з боку Росії.\n",
        "Культура України є унікальною та різноманітною, зі своїми традиціями, музикою, літературою та мистецтвом.\n",
        "Українська мова є єдиною державною мовою.\n",
        "\"\"\"\n",
        "\n",
        "# Підсумувати український текст до 3 речень\n",
        "summary_ukr = summarize_text(long_text_ukrainian, language=\"english\", sentences_count=3) # Вказуйте мову, яка відповідає тексту\n",
        "print(\"--- Підсумок українського тексту (3 речення) ---\")\n",
        "print(summary_ukr)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "long_text_english = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer science,\n",
        "and artificial intelligence concerned with the interactions between computers and human language,\n",
        "in particular how to program computers to process and analyze large amounts of natural language data.\n",
        "Challenges in natural language processing frequently involve speech recognition,\n",
        "natural language understanding, and natural language generation.\n",
        "NLP is a key technology behind many modern applications, including virtual assistants,\n",
        "machine translation, sentiment analysis, and text summarization.\n",
        "The field has seen significant advancements in recent years due to the rise of deep learning\n",
        "and the availability of large datasets.\n",
        "Many businesses are now leveraging NLP to extract insights from customer feedback,\n",
        "automate customer service, and improve search capabilities.\n",
        "Research continues to focus on improving the accuracy and efficiency of NLP models\n",
        "across a wide range of languages and tasks.\n",
        "\"\"\"\n",
        "\n",
        "# Підсумувати англійський текст до 4 речень\n",
        "summary_eng = summarize_text(long_text_english, language=\"english\", sentences_count=4)\n",
        "print(\"--- Підсумок англійського тексту (4 речення) ---\")\n",
        "print(summary_eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WOoIqwAakHS",
        "outputId": "d87516be-3efd-4418-8230-e862239402fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Підсумок українського тексту (3 речення) ---\n",
            "Україна - найбільша за площею країна Східної Європи, розташована в південно-східній частині континенту. Її територія охоплює значну частину Східноєвропейської рівнини, частину Карпатських гір та Кримського півострова. Столицею та найбільшим містом України є Київ.\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Підсумок англійського тексту (4 речення) ---\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. NLP is a key technology behind many modern applications, including virtual assistants, machine translation, sentiment analysis, and text summarization. The field has seen significant advancements in recent years due to the rise of deep learning and the availability of large datasets.\n"
          ]
        }
      ]
    }
  ]
}